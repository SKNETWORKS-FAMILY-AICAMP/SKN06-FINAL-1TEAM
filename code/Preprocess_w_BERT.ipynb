{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My Drive에 업로드해야 할 폴더 및 파일 <br/> : checkpoints 폴더, kpfbert 폴더, kpfSBERT 폴더, predict_module.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google colab에서 실행 시\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas scipy swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymysql sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import kss\n",
    "import torch\n",
    "import swifter\n",
    "import pymysql\n",
    "import logging\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from dotenv import load_dotenv\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.mysql import insert\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 경로 추가(predice_module.py 경로 지정 위함)\n",
    "sys.path.append(\"/content/drive/My Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: GPU available: False, used: False\n",
      "[Kss]: TPU available: False, using: 0 TPU cores\n",
      "[Kss]: HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 기사 요약 - KPF-BERTSum model\n",
    "from predict_module import summarize_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"현재 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# KPF-SBERT 모델 로드(google drive)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/kpfSBERT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m      4\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모델 경로 없음: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     exit()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# KPF-SBERT 모델 로드(google drive)\n",
    "model_path = \"/content/drive/MyDrive/kpfSBERT\"\n",
    "if not os.path.exists(model_path):\n",
    "    logging.error(f\"모델 경로 없음: {model_path}\")\n",
    "    exit()\n",
    "\n",
    "model = SentenceTransformer(model_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL 연결 설정\n",
    "db_url_mysql = os.getenv(\"MYSQL_URL\")\n",
    "engine = create_engine(db_url_mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 `cleaned_news_data` 테이블에서 데이터 불러오기\n",
    "query_existing = \"SELECT id FROM cleaned_news_data\"\n",
    "df_existing = pd.read_sql(query_existing, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 ID 목록\n",
    "existing_ids = set(df_existing[\"id\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로 가져온 뉴스 데이터\n",
    "query_new = \"SELECT id_org, title_org, pub_date_org, newspaper_org, content_org, link_org FROM news_data\"\n",
    "df = pd.read_sql(query_new, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존에 없는 새로운 데이터만 필터링\n",
    "df_new = df[~df[\"id_org\"].isin(existing_ids)].copy()\n",
    "\n",
    "if df_new.empty:\n",
    "    print(\"새로운 데이터가 없습니다.\")\n",
    "else:\n",
    "    print(f\"새로운 데이터 개수: {len(df_new)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 불필요한 단어 목록\n",
    "UNWANTED_PATTERNS = [\n",
    "    # 'ㅇㅇㅇ기자' 형태 제거\n",
    "    r\"\\b\\w+기자\\b\",\n",
    "    r\"\\b\\w{2,5}기자\\b\",\n",
    "    r\"\\b\\w+\\s*기자\\b\",\n",
    "    # '구독 구독중' 제거\n",
    "    r\"구독\\s*구독중\",\n",
    "    # '이전 다음 이미지확대' 제거\n",
    "    r\"이전\\s*다음\\s*이미지확대\",\n",
    "    # '이전 다음' 제거\n",
    "    r\"이전\\s*다음\",\n",
    "    # '이전 다음 이미지 확대' 제거\n",
    "    r\"이전\\s*다음\\s*이미지\\s*확대?\",\n",
    "    # '동영상 고정 취소' 제거\n",
    "    r\"동영상\\s*고정\\s*취소\",\n",
    "    # '사진게티이미지' 제거\n",
    "    r\"사진게티이미지\",\n",
    "    r\"사진\\s*게티이미지\",\n",
    "    r\"사진\\s*이베이\",\n",
    "    # 'viewer' 제거\n",
    "    r\"viewer\",\n",
    "    # '사진 확대' 제거\n",
    "    r\"사진\\s*확대\",\n",
    "    # 읽어주기 기능 관련 메시지 제거\n",
    "    r\"읽어주기\\s*기능은\\s*크롬기반의\\s*브라우저에서만\\s*사용하실\\s*수\\s*있습니다\\s*.\",\n",
    "    # '전체재생' 제거\n",
    "    r\"전체재생\"\n",
    "    # \"서울경제, 무단 전재 및 재배포 금지\" 단독 문구 제거\n",
    "    r\"서울경제,\\s*무단\\s*전재\\s*및\\s*재배포\\s*금지\",\n",
    "    # \"저작권자 ⓒ 신문사명, 무단 전재 및 재배포 금지\" 문구 삭제\n",
    "    r\"<*\\s*저작권자\\s*ⓒ?\\s*[가-힣A-Za-z\\s]+,\\s*무단\\s*전재\\s*및\\s*재배포\\s*금지\\s*>*\",\n",
    "    # 모든 '사진 제공 OO' 패턴 삭제 (괄호 포함 경우도 고려)\n",
    "    r\"\\(?사진\\s*제공\\s*[=:]?\\s*[가-힣A-Za-z]+\\)?\",\n",
    "    # \"사진 제공\" 다음 단어가 OO일 경우 제거 (ex: \"사진 제공 경기도\")\n",
    "    r\"\\b사진 제공 \\b[가-힣A-Za-z]+\\b\",\n",
    "    r\"전체재생\",\n",
    "    r\"앵커\",\n",
    "    r\"사진=게티이미지뱅크\",\n",
    "    r\"사진=연합뉴스\",\n",
    "    r\"\\(사진\\)\",\n",
    "    r\"한 눈에 읽기\",\n",
    "    r\"뉴스 요약쏙 AI 요약은 OpenAI의 최신 기술을 활용해 핵심 내용을 빠르고 정확하게 제공합니다\\. 전체 맥락을 이해하려면 기사 본문을 함께 확인하는 것이 좋습니다\\.\",\n",
    "    r\"공유\",\n",
    "    r\"이메일\",\n",
    "    r\"기사저장\",\n",
    "    r\"이 기사와 관련된 기사\",\n",
    "    r\"무단전재재배포 금지\",\n",
    "    r\"저작권자 파이낸셜뉴스\",\n",
    "    r\"AD 투자가를 위한 경제콘텐츠 플랫폼\",\n",
    "    r\"무단전재 배포금지\",\n",
    "    r\"아시아경제www\\.asiae\\.co\\.kr\",\n",
    "    r\"파이낸셜뉴스\",\n",
    "    r\"사진뉴스1\",\n",
    "    r\"연합뉴스\",\n",
    "    r\"제보하기\",\n",
    "    r\"채널 추가 전화 \\d{4,}\",\n",
    "    r\"김광수 특파원의 ‘중알중알’은 ‘중국을 알고 싶어 중국을 알려줄게’의 줄임말입니다\\..*?구독을 하시면 매주 금요일 유익한 중국 정보를 전달받으실 수 있습니다\\.\",\n",
    "    r\"헤럴드경제\",\n",
    "    r\"\\*\\s*편집자\\s*주:\\s*‘AI\\s*PRISM’.*?제공합니다\\.\",\n",
    "    r\"\\*\\s*편집자\\s*주\\s*:\\s*‘AI\\s*PRISM’.*?제공합니다\\.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 전처리 함수\n",
    "def clean_text(text):\n",
    "    for pattern in UNWANTED_PATTERNS:\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "        \n",
    "    # 특수문자 제거(마침표, 쉼표, 작은 따옴표, 큰 따옴표, 퍼센트센트 유지)\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9.,'\\\"%p\\s‘’“”△:()]\", \"\", text)\n",
    "    # 숫자와 단위(조, 원, %, p) 사이 공백 유지\n",
    "    text = re.sub(r\"(\\d+)\\s+(조|원|%|p)\", r\"\\1\\2\", text)\n",
    "    # 연속공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # 대괄호([]) 포함 내용 제거\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    # 온점(`.`) 뒤에 공백 추가 (이미 공백이 있으면 유지)\n",
    "    text = re.sub(r\"\\.(?!\\s|$)\", \". \", text)\n",
    "    # `△`를 쉼표(`,`)로 변경하고 앞 공백 제거, 뒤에 공백 추가\n",
    "    text = re.sub(r\"\\s*△\\s*\", \", \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 본문 전처리 적용\n",
    "df_new[\"content_display\"] = df_new[\"content_org\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목 전처리 함수\n",
    "def clean_title(text):\n",
    "    for pattern in UNWANTED_PATTERNS:\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "\n",
    "    # 대괄호([]) 포함 내용 삭제\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    # 특수문자 제거\n",
    "    ttext = re.sub(r\"[^가-힣a-zA-Z0-9.,'\\\"%p\\s‘’“”:()]\", \"\", text)\n",
    "    # 연속공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 제목 전처리 적용\n",
    "df_new[\"title\"] = df_new[\"title_org\"].apply(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 ID 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 중복된 id_org 값 확인\n",
    "duplicate_ids = df[\"id_org\"].duplicated().sum()\n",
    "print(f\"중복된 ID 개수: {duplicate_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 컬럼 선택 및 NULL 값 추가\n",
    "df_insert = df[[\"id_org\", \"title_org\", \"title\", \"pub_date_org\", \"newspaper_org\", \"content_display\", \"link_org\"]].copy()\n",
    "df_insert[\"content_summary\"] = \"\"  # 추후 데이터 추가 예정\n",
    "df_insert[\"keyword_5\"] = \"\"  # 추후 데이터 추가 예정\n",
    "df_insert[\"keyword_MMR\"] = \"\"  # 추후 데이터 추가 예정\n",
    "df_insert[\"sentiment\"] = None # 추후 데이터 추가 예정\n",
    "df_insert[\"sentiment_gpt\"] = None # 추후 데이터 추가 예정\n",
    "\n",
    "# 컬럼명 변경\n",
    "df_insert.rename(columns={\"id_org\": \"id\", \"pub_date_org\": \"pub_date\"}, inplace=True)\n",
    "\n",
    "# 데이터 삽입\n",
    "with engine.begin() as conn:\n",
    "    df_insert.to_sql(\"cleaned_news_data\", conn, if_exists=\"append\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 형태소 분석 및 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 불러오기\n",
    "## keyword_5(코사인 유사도 계산)와 keyword_MMR(MMR 알고리즘) 변경\n",
    "query = \"SELECT id, title, content_display, keyword_5 FROM cleaned_news_data WHERE keyword_5 = ''\"\n",
    "df = pd.read_sql(query, engine)  # 빈 문자열 값이 있는 데이터만 불러오기\n",
    "\n",
    "\n",
    "# 데이터 개수 체크\n",
    "print(\"쿼리 결과 개수:\", len(df))\n",
    "if df.empty:\n",
    "    print(\"업데이트할 데이터가 없습니다. 실행을 중단합니다.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트\n",
    "STOPWORDS = {\"없다\", \"없는\", \"없습니다\", \"않다\", \"아무\", \"없이\", \"없다는\", \"없었다\", \"모르다\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "StatementError",
     "evalue": "(sqlalchemy.exc.InvalidRequestError) A value is required for bind parameter 'keyword'\n[SQL: \n        UPDATE cleaned_news_data\n        SET keyword_5 = %(keyword)s\n        WHERE id = %(id)s\n        ]\n(Background on this error at: https://sqlalche.me/e/20/cd3x)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1815\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1813\u001b[0m         conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_revalidate_connection()\n\u001b[1;32m-> 1815\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (exc\u001b[38;5;241m.\u001b[39mPendingRollbackError, exc\u001b[38;5;241m.\u001b[39mResourceClosedError):\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:1390\u001b[0m, in \u001b[0;36mDefaultExecutionContext._init_compiled\u001b[1;34m(cls, dialect, connection, dbapi_connection, execution_options, compiled, parameters, invoked_statement, extracted_parameters, cache_hit)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameters:\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 1390\u001b[0m         \u001b[43mcompiled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextracted_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextracted_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mescape_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1394\u001b[0m     ]\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\sql\\compiler.py:1930\u001b[0m, in \u001b[0;36mSQLCompiler.construct_params\u001b[1;34m(self, params, extracted_parameters, escape_names, _group_number, _check, _no_postcompile)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mInvalidRequestError(\n\u001b[0;32m   1931\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA value is required for bind parameter \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1932\u001b[0m             \u001b[38;5;241m%\u001b[39m bindparam\u001b[38;5;241m.\u001b[39mkey,\n\u001b[0;32m   1933\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcd3x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1934\u001b[0m         )\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_extracted:\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: A value is required for bind parameter 'keyword' (Background on this error at: https://sqlalche.me/e/20/cd3x)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mStatementError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# SQL 실행 최적화\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mbegin() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m---> 55\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# `executemany()` 효과로 빠르게 실행\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keyword_5` 컬럼 업데이트 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1418\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py:515\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[1;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1640\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[1;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[0;32m   1628\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[0;32m   1630\u001b[0m )\n\u001b[0;32m   1632\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[0;32m   1633\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[0;32m   1634\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1638\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[0;32m   1639\u001b[0m )\n\u001b[1;32m-> 1640\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[0;32m   1654\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1655\u001b[0m         elem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m         ret,\n\u001b[0;32m   1660\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1821\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1821\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction\u001b[38;5;241m.\u001b[39mis_active\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     )\n\u001b[0;32m   1832\u001b[0m ):\n\u001b[0;32m   1833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalid_transaction()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2355\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2354\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2357\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1815\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1813\u001b[0m         conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_revalidate_connection()\n\u001b[1;32m-> 1815\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (exc\u001b[38;5;241m.\u001b[39mPendingRollbackError, exc\u001b[38;5;241m.\u001b[39mResourceClosedError):\n\u001b[0;32m   1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:1390\u001b[0m, in \u001b[0;36mDefaultExecutionContext._init_compiled\u001b[1;34m(cls, dialect, connection, dbapi_connection, execution_options, compiled, parameters, invoked_statement, extracted_parameters, cache_hit)\u001b[0m\n\u001b[0;32m   1382\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mInvalidRequestError(\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDialect \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdialect_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent server capabilities does not support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDELETE..RETURNING when executemany is used\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameters:\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 1390\u001b[0m         \u001b[43mcompiled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextracted_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextracted_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mescape_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1394\u001b[0m     ]\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1397\u001b[0m         compiled\u001b[38;5;241m.\u001b[39mconstruct_params(\n\u001b[0;32m   1398\u001b[0m             m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grp, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parameters)\n\u001b[0;32m   1404\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\miniconda3\\Lib\\site-packages\\sqlalchemy\\sql\\compiler.py:1930\u001b[0m, in \u001b[0;36mSQLCompiler.construct_params\u001b[1;34m(self, params, extracted_parameters, escape_names, _group_number, _check, _no_postcompile)\u001b[0m\n\u001b[0;32m   1923\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mInvalidRequestError(\n\u001b[0;32m   1924\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA value is required for bind parameter \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1925\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min parameter group \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1926\u001b[0m             \u001b[38;5;241m%\u001b[39m (bindparam\u001b[38;5;241m.\u001b[39mkey, _group_number),\n\u001b[0;32m   1927\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcd3x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1928\u001b[0m         )\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mInvalidRequestError(\n\u001b[0;32m   1931\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA value is required for bind parameter \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1932\u001b[0m             \u001b[38;5;241m%\u001b[39m bindparam\u001b[38;5;241m.\u001b[39mkey,\n\u001b[0;32m   1933\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcd3x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1934\u001b[0m         )\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_extracted:\n\u001b[0;32m   1937\u001b[0m     value_param \u001b[38;5;241m=\u001b[39m resolved_extracted\u001b[38;5;241m.\u001b[39mget(bindparam, bindparam)\n",
      "\u001b[1;31mStatementError\u001b[0m: (sqlalchemy.exc.InvalidRequestError) A value is required for bind parameter 'keyword'\n[SQL: \n        UPDATE cleaned_news_data\n        SET keyword_5 = %(keyword)s\n        WHERE id = %(id)s\n        ]\n(Background on this error at: https://sqlalche.me/e/20/cd3x)"
     ]
    }
   ],
   "source": [
    "# < 코사인 유사도 기반 키워드 추출 - keyword_5 >\n",
    "def extract_keywords_with_embedding(title, content, top_n=5, threshold=0.4):\n",
    "    # 본문이 None 또는 NaN이면 빈 리스트 반환\n",
    "    if pd.isna(content) or not isinstance(content, str) or content.strip() == \"\":\n",
    "        return [\"국내 자동차\"]\n",
    "\n",
    "    words = [word for word, pos in okt.pos(content) if pos in [\"Noun\"] and len(word) > 1 and word not in STOPWORDS]\n",
    "\n",
    "    # 출현 빈도를 고려하여 중요한 단어 우선 선택\n",
    "    word_freq = Counter(words)\n",
    "    sorted_words = [word for word, freq in word_freq.most_common(50)]  # 최대 50개 단어 사용\n",
    "\n",
    "    if not sorted_words:\n",
    "        return [\"국내 자동차\"]\n",
    "\n",
    "    # 제목과 키워드를 한 번에 벡터화하여 성능 최적화\n",
    "    sentences = [title] + sorted_words\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True, device=\"cuda\")\n",
    "\n",
    "    title_embedding = embeddings[0].cpu()\n",
    "    word_embeddings = embeddings[1:].cpu()\n",
    "\n",
    "    similarities = util.pytorch_cos_sim(title_embedding, word_embeddings).squeeze(0)\n",
    "    filtered_keywords = [\n",
    "        sorted_words[i] for i in similarities.argsort(descending=True).tolist() if similarities[i] >= threshold\n",
    "    ]\n",
    "\n",
    "    unique_keywords = list(dict.fromkeys(filtered_keywords))[:top_n]\n",
    "    unique_keywords = [word for word in unique_keywords if not word.replace(\"%\", \"\").isdigit()]\n",
    "    if not unique_keywords:\n",
    "        return [\"국내 자동차\"]\n",
    "\n",
    "    return unique_keywords\n",
    "\n",
    "# 키워드 추출 실행 (병렬 처리 적용)\n",
    "df[\"keyword_5\"] = df.swifter.apply(\n",
    "    lambda row: \", \".join(\n",
    "        extract_keywords_with_embedding(str(row[\"title\"]), str(row[\"content_display\"]), top_n=5, threshold=0.4)\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# `None` 또는 NaN을 빈 문자열(\"\")로 변경 (최종 보정)\n",
    "df[\"keyword_5\"] = df[\"keyword_5\"].fillna(\"\").astype(str)\n",
    "\n",
    "# 업데이트할 데이터 리스트 생성 (`None`을 빈 문자열로 대체)\n",
    "update_data = [\n",
    "    {\"keyword\": row[\"keyword_5\"] if row[\"keyword_5\"] else \"\", \"id\": row[\"id\"]}\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# `update_data`가 비어있는 경우 실행 안 함\n",
    "if not update_data:\n",
    "    print(\"업데이트할 데이터가 없습니다. 실행을 중단합니다.\")\n",
    "    sys.exit(0) \n",
    "\n",
    "print(\"업데이트할 데이터 샘플:\", update_data[:5])  # 일부 데이터 출력\n",
    "\n",
    "# 키워드 저장 - 기존 테이블 업데이트\n",
    "sql =  text(\"\"\"\n",
    "        UPDATE cleaned_news_data\n",
    "        SET keyword_5 = :keyword\n",
    "        WHERE id = :id\n",
    "        \"\"\")\n",
    "\n",
    "# SQL 실행 최적화\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sql, update_data) \n",
    "\n",
    "print(\"`keyword_5` 컬럼 업데이트 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 불러오기\n",
    "## keyword_5(코사인 유사도 계산)와 keyword_MMR(MMR 알고리즘) 변경\n",
    "query = \"SELECT id, title, content_display, keyword_MMR FROM cleaned_news_data WHERE keyword_MMR = ''\"\n",
    "df = pd.read_sql(query, engine)  # 빈 문자열 값이 있는 데이터만 불러오기\n",
    "\n",
    "\n",
    "# 데이터 개수 체크\n",
    "print(\"쿼리 결과 개수:\", len(df))\n",
    "if df.empty:\n",
    "    print(\"업데이트할 데이터가 없습니다. 실행을 중단합니다.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트\n",
    "STOPWORDS = {\"없다\", \"없는\", \"없습니다\", \"않다\", \"아무\", \"없이\", \"없다는\", \"없었다\", \"모르다\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < MMR(Maximal Marginal Relevance) 알고리즘 기반 키워드 추출 - keyword_MMR >\n",
    "\n",
    "def extract_keywords_with_mmr(title, content, top_n=5, diversity=0.7):\n",
    "    # 본문이 None 또는 NaN이면 빈 리스트 반환\n",
    "    if pd.isna(content) or not isinstance(content, str) or content.strip() == \"\":\n",
    "        return [\"국내 자동차\"]\n",
    "\n",
    "    words = [word for word, pos in okt.pos(content) if pos in [\"Noun\"] and len(word) > 1 and word not in STOPWORDS]\n",
    "\n",
    "    # 출현 빈도를 고려하여 중요한 단어 우선 선택 - 최대 50개\n",
    "    word_freq = Counter(words)\n",
    "    sorted_words = [word for word, freq in word_freq.most_common(50)]\n",
    "\n",
    "    # 단어가 1개 이하라면 기본 키워드 반환\n",
    "    if len(sorted_words) < 2:\n",
    "        return [\"국내 자동차\"]\n",
    "\n",
    "    # 제목과 키워드를 한 번에 벡터화하여 성능 최적화\n",
    "    sentences = [title] + sorted_words\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True, device=device)\n",
    "\n",
    "    title_embedding = embeddings[0]\n",
    "    word_embeddings = embeddings[1:]\n",
    "\n",
    "    # 제목과 단어 간 유사도 계산(Relevance)\n",
    "    word_similarities = util.pytorch_cos_sim(title_embedding, word_embeddings).squeeze(0)\n",
    "\n",
    "    # MMR 알고리즘 적용\n",
    "    selected_keywords = []\n",
    "    selected_indices = set()\n",
    "\n",
    "    for _ in range(min(top_n, len(sorted_words))):\n",
    "        candidates = [\n",
    "            (idx, word_similarities[idx].item())\n",
    "            for idx in range(len(sorted_words))\n",
    "            if idx not in selected_indices\n",
    "        ]\n",
    "\n",
    "        # 선택할 단어가 없을 시 종료\n",
    "        if not candidates:\n",
    "            break\n",
    "\n",
    "        if not selected_keywords:\n",
    "            # Relevance 기준 가장 유사한 키워드 선택\n",
    "            best_idx = max(candidates, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            # Diversity를 고려하여 MMR Score 계산\n",
    "            if selected_indices:  # 선택된 키워드가 있을 때만 Diversity 계산\n",
    "                similarity_scores = torch.stack(\n",
    "                    [util.pytorch_cos_sim(word_embeddings[i], word_embeddings).squeeze(0) for i in selected_indices]\n",
    "                )\n",
    "                diversity_score = torch.mean(similarity_scores, dim=0)  # 평균 계산\n",
    "            else:\n",
    "                diversity_score = torch.zeros_like(word_similarities)\n",
    "\n",
    "            mmr_score = diversity * word_similarities - (1 - diversity) * diversity_score\n",
    "\n",
    "            # 벡터에서 값 추출 시 `.item()` 적용 방식 수정\n",
    "            mmr_candidates = [\n",
    "                (idx, float(mmr_score[idx]))  # 텐서에서 값 추출\n",
    "                for idx in range(len(sorted_words))\n",
    "                if idx not in selected_indices\n",
    "            ]\n",
    "\n",
    "            best_idx = max(mmr_candidates, key=lambda x: x[1])[0]\n",
    "\n",
    "        selected_indices.add(best_idx)\n",
    "        selected_keywords.append(sorted_words[best_idx])\n",
    "\n",
    "    # 중복 제거 후 반환\n",
    "    return list(set(selected_keywords)) if selected_keywords else [\"국내 자동차\"]\n",
    "\n",
    "# 기존 cleaned_news_data 테이블에서 id, title, content_display 컬럼 불러오기\n",
    "query = \"\"\"\n",
    "SELECT id, title, content_display, keyword_MMR\n",
    "FROM cleaned_news_data\n",
    "WHERE keyword_MMR = '' OR keyword_MMR IS NULL\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# 병렬 처리로 MMR 키워드 추출 적용\n",
    "df[\"keyword_MMR\"] = df.swifter.apply(\n",
    "    lambda row: \", \".join(\n",
    "        extract_keywords_with_mmr(str(row[\"title\"]), str(row[\"content_display\"]), top_n=5, diversity=0.7)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# MySQL에 저장\n",
    "sql = text(\"\"\"\n",
    "            UPDATE cleaned_news_data\n",
    "            SET keyword_MMR = :keyword_MMR\n",
    "            WHERE id = :id\n",
    "            \"\"\")\n",
    "\n",
    "update_data = []\n",
    "for _, row in df.iterrows():\n",
    "    keyword_mmr = row[\"keyword_MMR\"]\n",
    "\n",
    "    # 빈 문자열이면 기본값 \"국내 자동차\"로 변경\n",
    "    if keyword_mmr == \"\" or keyword_mmr is None or pd.isna(keyword_mmr):\n",
    "        keyword_mmr = \"국내 자동차\"\n",
    "\n",
    "    update_data.append({\"keyword_MMR\": keyword_mmr, \"id\": row[\"id\"]})\n",
    "\n",
    "# 디버깅용 업데이트할 데이터 개수 출력\n",
    "print(f\"업데이트할 데이터 개수: {len(update_data)}\")\n",
    "\n",
    "if update_data:\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(sql, update_data)\n",
    "\n",
    "    print(\"keyword_MMR 컬럼 업데이트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 키워드 추출 성능 평가 지표 <br/> 뉴스 주제별 연관된 키워드를 선택하고 싶으면 코사인 유사도 방식 <br/> 더 다양한 키워드를 제공하려면 MMR 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드 추출 성능 평가 지표\n",
    "## 키워드 다양성 평가(Diversity Score) - 키워드 간 평균 코사인 유사도 측정. 낮을수록 MMR 효과적.\n",
    "\n",
    "# Sentence-BERT 모델 로드\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "def calculate_diversity_score(keyword_list):\n",
    "    \"\"\"\n",
    "    키워드 리스트를 입력받아 다양성을 평가하는 함수.\n",
    "    코사인 유사도의 평균값을 Diversity Score로 사용.\n",
    "    \"\"\"\n",
    "    if len(keyword_list) < 2:\n",
    "        return 1.0  # 키워드가 1개 이하일 경우, 다양성이 없는 것으로 간주하여 1.0 반환\n",
    "\n",
    "    # 키워드 임베딩 생성\n",
    "    keyword_embeddings = model.encode(keyword_list, convert_to_tensor=True)\n",
    "    \n",
    "    # 키워드 간 코사인 유사도 계산\n",
    "    similarity_matrix = util.pytorch_cos_sim(keyword_embeddings, keyword_embeddings)\n",
    "\n",
    "    # 상삼각행렬(triu)에서 대각선 제외 후 평균 계산\n",
    "    num_keywords = len(keyword_list)\n",
    "    diversity_score = (\n",
    "        torch.sum(similarity_matrix) - torch.sum(torch.diagonal(similarity_matrix))\n",
    "    ) / (num_keywords * (num_keywords - 1))\n",
    "\n",
    "    return diversity_score.item()  # 낮을수록 다양한 키워드가 선택됨\n",
    "\n",
    "# 예제 키워드 비교\n",
    "keywords_cosine = [\"자동차\", \"전기차\", \"내연기관\", \"배터리\", \"충전\"]\n",
    "keywords_mmr = [\"자동차\", \"수출\", \"시장\", \"친환경\", \"신차\"]\n",
    "\n",
    "print(f\"코사인 유사도 기반 키워드 다양성 점수: {calculate_diversity_score(keywords_cosine):.4f}\")\n",
    "print(f\"MMR 기반 키워드 다양성 점수: {calculate_diversity_score(keywords_mmr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 기사 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# GPU 환경(Google Colab)\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 함께 드라이브에 업로드\n",
    "# predict_module.py 파일\n",
    "# kpfbert 파일\n",
    "# checkpoints 폴더\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPF-BERT 모델 로드(google drive)\n",
    "model_path = \"/content/drive/MyDrive/kpfbert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로가 제대로 설정되었는지 확인\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"BERT 모델 경로 확인 완료: {model_path}\")\n",
    "else:\n",
    "    print(f\"BERT 모델 경로 오류: {model_path}가 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "bert_model = BertModel.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"BERT 모델 및 Tokenizer 로드 성공\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL 연결 설정\n",
    "db_url_mysql = os.getenv(\"MYSQL_URL\")\n",
    "engine = create_engine(db_url_mysql, pool_recycle=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL 연결 테스트\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM cleaned_news_data\"))\n",
    "        count = result.scalar()  \n",
    "        print(f\"데이터 개수: {count} 개\")\n",
    "except Exception as e:\n",
    "    print(f\"MySQL 연결 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter 설정\n",
    "MAX_TOKEN_COUNT = 512\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL에서 뉴스 기사 본문 데이터 가져오기\n",
    "query = \"SELECT content_display FROM cleaned_news_data\"\n",
    "df = pd.read_sql(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 개수 1084\n"
     ]
    }
   ],
   "source": [
    "# 결측치 제거\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"전체 데이터 개수 {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 - 학습/검증/테스트\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 초기화\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터(test_df) 확보 (최대 100개)\n",
    "test_df = val_df[:100] if len(val_df) > 100 else val_df.copy()\n",
    "val_df = val_df[100:] if len(val_df) > 100 else val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape(학습): (867, 1), test_df.shape(테스트): (100, 1), val_df.shape(검증): (117, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 크기 출력\n",
    "print(f\"train_df.shape(학습): {train_df.shape}, test_df.shape(테스트): {test_df.shape}, val_df.shape(검증): {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    outs = []\n",
    "    for idx, row in data.iterrows():\n",
    "        # 문장 단위 분리\n",
    "        article_original = kss.split_sentences(row['content_display'])\n",
    "        outs.append([article_original])\n",
    "\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병렬 처리 문장 분리\n",
    "import multiprocessing\n",
    "\n",
    "def split_sentences_parallel(texts):\n",
    "    with multiprocessing.Pool(processes=os.cpu_count()) as pool:\n",
    "        result = pool.map(kss.split_sentences, texts)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 # 경고 메시지 무시 가능\n",
    "train_df['article_original'] = split_sentences_parallel(train_df['content_display'])\n",
    "test_df['article_original'] = split_sentences_parallel(test_df['content_display'])\n",
    "val_df['article_original'] = split_sentences_parallel(val_df['content_display'])\n",
    "\n",
    "print(\"데이터 전처리 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset(Presumm에서 제안한 형식으로 인코딩 - bert에서 여러 문장을 입력하기 위해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummDataset(Dataset): # 데이터셋 presumm 방식 인코딩\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: BertTokenizer,\n",
    "        max_token_len: int = 512\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        tokenlist = []\n",
    "        for sent in data_row.article_original:\n",
    "            tokenlist.append(self.tokenizer(\n",
    "                text = sent,\n",
    "                add_special_tokens = True))\n",
    "\n",
    "        src = [] # 토크나이징 된 전체 문단\n",
    "        segs = []  #각 토큰에 대해 홀수번째 문장이면 0, 짝수번째 문장이면 1을 매핑\n",
    "        clss = []  #[CLS]토큰의 포지션값을 지정\n",
    "        labels = [] #문장별 요약 여부(요약이면 1, 아니면 0)\n",
    "\n",
    "        odd = 0\n",
    "        for tkns in tokenlist:\n",
    "            if odd > 1:\n",
    "                odd = 0\n",
    "            clss.append(len(src))\n",
    "            src.extend(tkns['input_ids'])\n",
    "            segs.extend([odd] * len(tkns['input_ids']))\n",
    "\n",
    "            # labels 추가\n",
    "            if 'extractive' in data_row and isinstance(data_row.extractive, list):\n",
    "                labels.append(1 if tokenlist.index(tkns) in data_row.extractive else 0)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            odd += 1\n",
    "\n",
    "            #truncation\n",
    "            if len(src) >= self.max_token_len:\n",
    "                src = src[:self.max_token_len - 1] + [src[-1]]\n",
    "                segs = segs[:self.max_token_len]\n",
    "                break\n",
    "\n",
    "        #padding\n",
    "        pad_len = self.max_token_len - len(src)\n",
    "        src.extend([0] * pad_len)\n",
    "        segs.extend([0] * pad_len)\n",
    "        clss.extend([-1] * (self.max_token_len - len(clss)))\n",
    "        labels.extend([0] * (self.max_token_len - len(labels)))\n",
    "\n",
    "        return dict(\n",
    "            src=torch.tensor(src),\n",
    "            segs=torch.tensor(segs),\n",
    "            clss=torch.tensor(clss),\n",
    "            labels=torch.tensor(labels, dtype=torch.float)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummDataModule(pl.LightningDataModule): # presumm 인코딩 모듈\n",
    "\n",
    "    def __init__(self, train_df, test_df, val_df, tokenizer, batch_size=1, max_token_len=512):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.val_df = val_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = SummDataset(self.train_df, self.tokenizer, self.max_token_len)\n",
    "        self.test_dataset = SummDataset(self.test_df, self.tokenizer, self.max_token_len)\n",
    "        self.val_dataset = SummDataset(self.val_df, self.tokenizer, self.max_token_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule 생성\n",
    "data_module = SummDataModule(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    val_df,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "print(\"데이터 로더 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader()에서 labels 포함 여부 확인\n",
    "sample_loader = iter(data_module.train_dataloader())\n",
    "sample_batch = next(sample_loader)\n",
    "\n",
    "print(sample_batch.keys())  # 'labels'가 포함되어 있는지 확인\n",
    "print(sample_batch['labels'].shape)  # labels의 형태 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model <br/> (kpfBERT를 pretrained_bert로 불러와서 후처리 레이어 추가 후 문장 추출 모델 만듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): # positional embedding\n",
    "\n",
    "    def __init__(self, dropout, dim, max_len=5000):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                            -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        if (step):\n",
    "            emb = emb + self.pe[:, step][:, None, :]\n",
    "\n",
    "        else:\n",
    "            emb = emb + self.pe[:, :emb.size(1)]\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n",
    "\n",
    "    def get_emb(self, emb):\n",
    "        return self.pe[:, :emb.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadedAttention(\n",
    "            heads, d_model, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, iter, query, inputs, mask):\n",
    "        if (iter != 0):\n",
    "            input_norm = self.layer_norm(inputs)\n",
    "        else:\n",
    "            input_norm = inputs\n",
    "\n",
    "        mask = mask.unsqueeze(1)\n",
    "        context = self.self_attn(input_norm, input_norm, input_norm, mask=mask)\n",
    "        out = self.dropout(context) + inputs\n",
    "        return self.feed_forward(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtTransformerEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, d_ff=2048, heads=8, dropout=0.2, num_inter_layers=2):\n",
    "        super(ExtTransformerEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_inter_layers = num_inter_layers\n",
    "        self.pos_emb = PositionalEncoding(dropout, hidden_size)\n",
    "        self.transformer_inter = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(hidden_size, heads, d_ff, dropout)\n",
    "            for _ in range(num_inter_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.wo = nn.Linear(hidden_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, top_vecs, mask):\n",
    "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
    "\n",
    "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
    "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
    "        x = top_vecs * mask[:, :, None].float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for i in range(self.num_inter_layers):\n",
    "            x = self.transformer_inter[i](i, x, x, ~mask.bool())\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        sent_scores = self.sigmoid(self.wo(x))\n",
    "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
    "\n",
    "        return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\" A two-layer Feed-Forward-Network with residual layer norm.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): the size of input for the first-layer of the FFN.\n",
    "        d_ff (int): the hidden layer size of the second-layer\n",
    "            of the FNN.\n",
    "        dropout (float): dropout probability in :math:`[0, 1)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inter = self.dropout_1(self.gelu(self.w_1(self.layer_norm(x))))\n",
    "        output = self.dropout_2(self.w_2(inter))\n",
    "        return output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module from\n",
    "    \"Attention is All You Need\"\n",
    "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n",
    "\n",
    "    Similar to standard `dot` attention but uses\n",
    "    multiple attention distributions simulataneously\n",
    "    to select relevant items.\n",
    "\n",
    "    .. mermaid::\n",
    "\n",
    "    graph BT\n",
    "        A[key]\n",
    "        B[value]\n",
    "        C[query]\n",
    "        O[output]\n",
    "        subgraph Attn\n",
    "            D[Attn 1]\n",
    "            E[Attn 2]\n",
    "            F[Attn N]\n",
    "        end\n",
    "        A --> D\n",
    "        C --> D\n",
    "        A --> E\n",
    "        C --> E\n",
    "        A --> F\n",
    "        C --> F\n",
    "        D --> O\n",
    "        E --> O\n",
    "        F --> O\n",
    "        B --> O\n",
    "\n",
    "    Also includes several additional tricks.\n",
    "\n",
    "    Args:\n",
    "    head_count (int): number of parallel heads\n",
    "    model_dim (int): the dimension of keys/values/queries,\n",
    "        must be divisible by head_count\n",
    "    dropout (float): dropout parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
    "        assert model_dim % head_count == 0\n",
    "        self.dim_per_head = model_dim // head_count\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_count = head_count\n",
    "\n",
    "        self.linear_keys = nn.Linear(model_dim,\n",
    "                                     head_count * self.dim_per_head)\n",
    "        self.linear_values = nn.Linear(model_dim,\n",
    "                                       head_count * self.dim_per_head)\n",
    "        self.linear_query = nn.Linear(model_dim,\n",
    "                                      head_count * self.dim_per_head)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.use_final_linear = use_final_linear\n",
    "        if (self.use_final_linear):\n",
    "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, key, value, query, mask=None,\n",
    "                layer_cache=None, type=None, predefined_graph_1=None):\n",
    "        \"\"\"\n",
    "        Compute the context vector and the attention vectors.\n",
    "\n",
    "        Args:\n",
    "        key (`FloatTensor`): set of `key_len`\n",
    "                key vectors `[batch, key_len, dim]`\n",
    "        value (`FloatTensor`): set of `key_len`\n",
    "                value vectors `[batch, key_len, dim]`\n",
    "        query (`FloatTensor`): set of `query_len`\n",
    "                query vectors  `[batch, query_len, dim]`\n",
    "        mask: binary mask indicating which keys have\n",
    "                non-zero attention `[batch, query_len, key_len]`\n",
    "        Returns:\n",
    "        (`FloatTensor`, `FloatTensor`) :\n",
    "\n",
    "           * output context vectors `[batch, query_len, dim]`\n",
    "           * one of the attention vectors `[batch, query_len, key_len]`\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = key.size(0)\n",
    "        dim_per_head = self.dim_per_head\n",
    "        head_count = self.head_count\n",
    "        key_len = key.size(1)\n",
    "        query_len = query.size(1)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"  projection \"\"\"\n",
    "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
    "                .transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"  compute context \"\"\"\n",
    "            return x.transpose(1, 2).contiguous() \\\n",
    "                .view(batch_size, -1, head_count * dim_per_head)\n",
    "\n",
    "        # 1) Project key, value, and query.\n",
    "        if layer_cache is not None:\n",
    "            if type == \"self\":\n",
    "                query, key, value = self.linear_query(query), \\\n",
    "                                    self.linear_keys(query), \\\n",
    "                                    self.linear_values(query)\n",
    "\n",
    "                key = shape(key)\n",
    "                value = shape(value)\n",
    "\n",
    "                if layer_cache is not None:\n",
    "                    device = key.device\n",
    "                    if layer_cache[\"self_keys\"] is not None:\n",
    "                        key = torch.cat(\n",
    "                            (layer_cache[\"self_keys\"].to(device), key),\n",
    "                            dim=2)\n",
    "                    if layer_cache[\"self_values\"] is not None:\n",
    "                        value = torch.cat(\n",
    "                            (layer_cache[\"self_values\"].to(device), value),\n",
    "                            dim=2)\n",
    "                    layer_cache[\"self_keys\"] = key\n",
    "                    layer_cache[\"self_values\"] = value\n",
    "            elif type == \"context\":\n",
    "                query = self.linear_query(query)\n",
    "                if layer_cache is not None:\n",
    "                    if layer_cache[\"memory_keys\"] is None:\n",
    "                        key, value = self.linear_keys(key), \\\n",
    "                                    self.linear_values(value)\n",
    "                        key = shape(key)\n",
    "                        value = shape(value)\n",
    "                    else:\n",
    "                        key, value = layer_cache[\"memory_keys\"], \\\n",
    "                                    layer_cache[\"memory_values\"]\n",
    "                    layer_cache[\"memory_keys\"] = key\n",
    "                    layer_cache[\"memory_values\"] = value\n",
    "                else:\n",
    "                    key, value = self.linear_keys(key), \\\n",
    "                                self.linear_values(value)\n",
    "                    key = shape(key)\n",
    "                    value = shape(value)\n",
    "        else:\n",
    "            key = self.linear_keys(key)\n",
    "            value = self.linear_values(value)\n",
    "            query = self.linear_query(query)\n",
    "            key = shape(key)\n",
    "            value = shape(value)\n",
    "\n",
    "        query = shape(query)\n",
    "\n",
    "        key_len = key.size(2)\n",
    "        query_len = query.size(2)\n",
    "\n",
    "        # 2) Calculate and scale scores.\n",
    "        query = query / math.sqrt(dim_per_head)\n",
    "        scores = torch.matmul(query, key.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(scores)\n",
    "            scores = scores.masked_fill(mask.bool(), -1e18)\n",
    "\n",
    "        # 3) Apply attention dropout and compute context vectors.\n",
    "\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        if (not predefined_graph_1 is None):\n",
    "            attn_masked = attn[:, -1] * predefined_graph_1\n",
    "            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
    "\n",
    "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
    "\n",
    "        drop_attn = self.dropout(attn)\n",
    "        if (self.use_final_linear):\n",
    "            context = unshape(torch.matmul(drop_attn, value))\n",
    "            output = self.final_linear(context)\n",
    "            return output\n",
    "        else:\n",
    "            context = torch.matmul(drop_attn, value)\n",
    "            return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(pl.LightningModule): # 요약 모델\n",
    "\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.max_pos = 512\n",
    "        self.bert = BertModel.from_pretrained(model_path, add_pooling_layer=False)\n",
    "        self.ext_layer = ExtTransformerEncoder()\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.loss = nn.BCELoss(reduction='none')\n",
    "        self.training_losses = []  # 학습 손실 저장\n",
    "        self.validation_losses = []  # 검증 손실 저장\n",
    "        self.test_losses = []  # 테스트 손실 저장\n",
    "\n",
    "        for p in self.ext_layer.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, segs, clss, labels=None): #, input_ids, attention_mask, labels=None):\n",
    "\n",
    "        mask_src = ~(src == 0).bool() # 1 - (src == 0)\n",
    "        mask_cls = ~(clss == -1).bool() # 1 - (clss == -1)\n",
    "\n",
    "        top_vec = self.bert(src, token_type_ids=segs, attention_mask=mask_src)\n",
    "        top_vec = top_vec.last_hidden_state\n",
    "\n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "\n",
    "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
    "\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.loss(sent_scores, labels)\n",
    "            loss = (loss * mask_cls.float()).sum() / len(labels)\n",
    "\n",
    "        return loss, sent_scores\n",
    "\n",
    "    def step(self, batch):\n",
    "        src = batch['src']\n",
    "        if len(batch['labels']) > 0 :\n",
    "            labels = batch['labels']\n",
    "        else:\n",
    "            labels = None\n",
    "        segs = batch['segs']\n",
    "        clss = batch['clss']\n",
    "\n",
    "        loss, sent_scores = self(src, segs, clss, labels)\n",
    "\n",
    "        return loss, sent_scores, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.training_losses.append(loss.item()) # 손실 저장\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.validation_losses.append(loss.item())\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.test_losses.append(loss.item())\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = sum(self.training_losses) / len(self.training_losses)\n",
    "        print(f\"Epoch {self.current_epoch} - Train Loss: {avg_loss}\")\n",
    "        self.log(\"avg_train_loss\", avg_loss, prog_bar=True, logger=True)\n",
    "        self.training_losses.clear()  # 손실 리스트 초기화\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = sum(self.validation_losses) / len(self.validation_losses)\n",
    "        print(f\"Epoch {self.current_epoch} - Validation Loss: {avg_loss}\")\n",
    "        self.log(\"avg_val_loss\", avg_loss, prog_bar=True, logger=True)\n",
    "        self.validation_losses.clear()  # 손실 리스트 초기화\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = sum(self.test_losses) / len(self.test_losses)\n",
    "        print(f\"Test Loss: {avg_loss}\")\n",
    "        self.log(\"avg_test_loss\", avg_loss, prog_bar=True, logger=True)\n",
    "        self.test_losses.clear()  # 손실 리스트 초기화\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "        if 'train_df' in globals():\n",
    "            steps_per_epoch = len(train_df) // BATCH_SIZE\n",
    "        else:\n",
    "            steps_per_epoch = 1  # 기본값 지정\n",
    "\n",
    "        total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=steps_per_epoch,\n",
    "            num_training_steps=total_training_steps\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab environment\n",
    "!rm -rf /content/lightning_logs/\n",
    "!rm -rf /content/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard 실행\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint 저장 설정\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # Colab에서의 절대 경로\n",
    "    dirpath=\"/content/checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"avg_val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard logger 설정\n",
    "logger = TensorBoardLogger(\"/content/lightning_logs\", name=\"kpfBERT_Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기 종료 설정\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='avg_val_loss',\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 설정\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1, # GPU 개수\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Trainer 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 실행\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loader = iter(data_module.train_dataloader())\n",
    "sample_batch = next(sample_loader)\n",
    "\n",
    "print(sample_batch.keys())  # 'labels'가 포함되어 있는지 확인\n",
    "print(sample_batch['labels'].shape)  # labels의 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 로드\n",
    "trained_model = Summarizer.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def data_process(text):\n",
    "    # 문장 분리\n",
    "    sents = kss.split_sentences(text.replace('\\n', ''))\n",
    "\n",
    "    #데이터 가공\n",
    "    tokenlist = [tokenizer(sent, add_special_tokens=True, max_length=MAX_TOKEN_COUNT, truncation=True) for sent in sents]\n",
    "\n",
    "    src, segs, clss = [], [], []\n",
    "    odd = 0\n",
    "\n",
    "    for tkns in tokenlist:\n",
    "\n",
    "        if odd > 1:\n",
    "            odd = 0\n",
    "        clss.append(len(src))\n",
    "        src.extend(tkns['input_ids'])\n",
    "        segs.extend([odd] * len(tkns['input_ids']))\n",
    "        odd += 1\n",
    "\n",
    "        #truncation\n",
    "        if len(src) >= MAX_TOKEN_COUNT:\n",
    "            src = src[:MAX_TOKEN_COUNT]  # 512개까지만 유지\n",
    "            segs = segs[:MAX_TOKEN_COUNT]\n",
    "            clss = clss[:MAX_TOKEN_COUNT]  # CLS 토큰도 동일하게 유지\n",
    "            break  # 최대 길이를 초과하면 중단\n",
    "\n",
    "    #padding\n",
    "    pad_len = MAX_TOKEN_COUNT - len(src)\n",
    "    src.extend([0] * pad_len)\n",
    "    segs.extend([0] * pad_len)\n",
    "    clss.extend([-1] * (MAX_TOKEN_COUNT - len(clss)))\n",
    "\n",
    "    return dict(\n",
    "        sents = sents, # 정답 출력을 위해 원문 문장 저장\n",
    "        src = torch.tensor(src),\n",
    "        segs = torch.tensor(segs),\n",
    "        clss = torch.tensor(clss),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 수행 함수\n",
    "def summarize_test(text):\n",
    "    data = data_process(text.replace('\\n', ''))\n",
    "\n",
    "    # 입력 데이터를 모델과 동일한 장치(GPU)로 이동\n",
    "    src = data['src'].unsqueeze(0).to(device)\n",
    "    segs = data['segs'].unsqueeze(0).to(device)\n",
    "    clss = data['clss'].unsqueeze(0).to(device)\n",
    "\n",
    "    #trained_model에 넣어 결과값 반환\n",
    "    _, rtn = trained_model(src, segs, clss)\n",
    "    rtn = rtn.squeeze()\n",
    "\n",
    "    # 예측 결과값을 받기 위한 프로세스\n",
    "    rtn_sort, idx = rtn.sort(descending=True)\n",
    "\n",
    "    rtn_sort = rtn_sort.tolist()\n",
    "    idx = idx.tolist()\n",
    "\n",
    "    # 0이 나오기 전까지 유효한 문장 선택\n",
    "    end_idx = rtn_sort.index(0)\n",
    "\n",
    "    rtn_sort = rtn_sort[:end_idx]\n",
    "    idx = idx[:end_idx]\n",
    "\n",
    "    # 최상위 3개 문장 선택\n",
    "    if len(idx) > 3:\n",
    "        rslt = idx[:3]\n",
    "    else:\n",
    "        rslt = idx\n",
    "\n",
    "    summ = []\n",
    "    print(' *** 입력한 문단의 요약문은 ...')\n",
    "    for i, r in enumerate(rslt):\n",
    "        summ.append(data['sents'][r])\n",
    "        print('[', i+1, ']', summ[i])\n",
    "\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 문장 입력\n",
    "test_context = '''이재명 더불어민주당 대선후보는 26일 변호사비 대납 의혹과 관련, \"내가 정말로 변호사비를 불법으로 받았으면 나를 구속하라\"고 반박했다.\n",
    "이 후보는 이날 오후 전남 신안군 응급의료 전용헬기 계류장에서 열린 '국민반상회' 후 기자들과 만나 한 시민단체 대표가 고액 수임료 의혹 증거라며 제시한 녹취록에 대해 \"조작됐다는 증거를 갖고 있고 검찰에도 제출했다. 검찰과 수사기관들은 빨리 처리하시라\"며 이같이 말했다.\n",
    "앞서 이민구 깨어있는시민연대당 대표는 이 후보가 특정 변호사에게 수임료로 현금과 주식 등 20억원을 줬다는 의혹을 주장하며 녹취록을 제출한 바 있다. 이에 대해 송평수 선대위 부대변인은 \"허위사실\"이라며 \"깨시민당 이 대표에게 제보를 했다는 시민단체 대표 이모 씨가 제3자로부터 기부금을 받아낼 목적으로 허위사실을 녹음한 후, 이 모 변호사에게까지 접근했다. 이러한 비상식적이고 악의적인 행태는 이재명 후보에 대한 정치적 타격을 가할 목적으로 치밀하게 준비한 것\"이라고 반박했다.\n",
    "이에 대해 이 후보는 \"그것도 조직폭력배 조작에 버금가는 조작사건이라는 게 곧 드러날 것\"이라며 \"팩트확인을 하고 언급하면 좋겠다. 당사자도 아니고 제3자들이 자기끼리 녹음한 게 가치가 있느냐\"고 반문했다.\n",
    "그는 \"사실이 아니면 무고하고 음해하는 사람들을 무고 혐의나 공직선거법 위반으로 빨리 처리해서 처벌하시라\"며 \"선거 국면에서 하루이틀도, 한두번도 아니고 '조폭이 뇌물 줬다'는 (허위사실 유포를) 왜 아직도 처리 안 하고 있느냐\"고 검경에 불만을 드러냈다.\n",
    "이어 \"허위사실이 드러났으면 당연히 다시는 그런 일이 없게 해야 하는 것 아닌가. 이해가 안 된다\"며 \"선거관리, 또는 범죄를 단속하는 국가기관들이 이런 식으로 허위사실 유포나 무고 행위를 방치해 정치적 공격 수단으로 쓰게 하면 안 된다\"고 했다.\n",
    "이 후보는 또 자신이 구민주-동교동계와 접촉해 복당을 타진했다는 언론보도와 관련해선 \"구체적으로 어떤 사람을 범주별로 나눠 무슨 계, 진영으로 말하는 것은 아니다\"라며 \"시점을 언젠가 정해 벌점이니, 제재니, 제한이니 다 없애고 모두가 합류할 수 있도록 할 생각\"이라고 말했다.\n",
    "종전에 언급했던 '대사면' 방침을 재확인한 셈이다. 그는 \"민주당에 계셨던 분, 또 민주당에 있지 않았더라도 앞으로 함께할 분들에게 계속 연락을 하고 있다\"며 \"만나고 전화하고 힘을 합치자고 권유하고 있다\"고 했다.\n",
    "그는 \" 현재 민주당이 이미 열린민주당과의 통합을 협의하고 있다\"며 \"거기에 더해서 꼭 민주계라고 말할 필요는 없고 부패사범이나 파렴치범으로 탈당하거나 또는 제명된 사람들이 아니라면, 국가의 미래를 걱정하는 민주개혁 진영의 일원이라면 가리지 말고 과거의 어떤 일이든 그러지 말고 힘을 합치자\"고 강조했다.\n",
    "언론보도에 따르면, 이 후보는 최근 구민주계인 정대철 전 고문과 연락을 주고 받으며 천정배, 정동영 전 의원 등 민주당을 탈당했던 옛 동교동계와 호남 인사들의 복당을 타진했다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_context 요약\n",
    "summary_context = summarize_test(test_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summarize_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m현대자동차는 전기차 생산 확대를 위해 신규 공장을 건설한다고 밝혔다. \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m이 공장은 2026년까지 완공될 예정이며, 연간 50만 대의 전기차를 생산할 계획이다.\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m또한, 현대차는 배터리 성능 향상을 위한 연구 개발도 적극적으로 추진하고 있다.\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m summary_result \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_test\u001b[49m(test_text)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔹 요약 결과:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(summary_result, \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summarize_test' is not defined"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "현대자동차는 전기차 생산 확대를 위해 신규 공장을 건설한다고 밝혔다.\n",
    "이 공장은 2026년까지 완공될 예정이며, 연간 50만 대의 전기차를 생산할 계획이다.\n",
    "또한, 현대차는 배터리 성능 향상을 위한 연구 개발도 적극적으로 추진하고 있다.\n",
    "\"\"\"\n",
    "\n",
    "summary_result = summarize_test(test_text)\n",
    "\n",
    "print(\"요약 결과:\")\n",
    "for idx, sentence in enumerate(summary_result, 1):\n",
    "    print(f\"{idx}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세션 재시작 시 필수 실행 코드"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
