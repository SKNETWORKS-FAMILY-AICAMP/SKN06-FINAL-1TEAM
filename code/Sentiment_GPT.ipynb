{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import openai\n",
    "import pymysql\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio ì˜¤ë¥˜ ë°©ì§€ íŒ¨í‚¤ì§€(Jupyter í™˜ê²½ì—ì„œ)\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ê°œìˆ˜: 1164 ê°œ\n"
     ]
    }
   ],
   "source": [
    "# MySQL ì—°ê²° ì„¤ì •\n",
    "db_url = os.getenv(\"MYSQL_URL\")\n",
    "engine = create_engine(db_url, pool_recycle=3600)\n",
    "\n",
    "# MySQL ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM cleaned_news_data\"))\n",
    "        count = result.scalar()\n",
    "        print(f\"âœ… ë°ì´í„° ê°œìˆ˜: {count} ê°œ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ MySQL ì—°ê²° ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—”ì§„ ìƒì„±\n",
    "engine = create_engine(db_url, pool_recycle=3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ê°ì • ë¶„ì„ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "query = \"\"\"SELECT id, content_display FROM cleaned_news_data WHERE sentiment_gpt IS NULL\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "print(f\"ê°ì • ë¶„ì„í•  ë°ì´í„° ê°œìˆ˜: {len(df)} ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"content_display\": \"content\"})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def batch_analyze_sentiment_gpt4o(news_list, timeout=30):\n",
    "    \"\"\"\n",
    "    GPT-4oë¥¼ ì´ìš©í•œ ê°ì • ë¶„ì„ (ë°°ì¹˜ ì²˜ë¦¬).\n",
    "    \"\"\"\n",
    "    prompt = \"\\n\\n\".join(\n",
    "        [f\"[ë‰´ìŠ¤ {i+1}]\\nì œëª©: {title}\\në³¸ë¬¸: {content}\" for i, (title, content) in enumerate(news_list)]\n",
    "    )\n",
    "    prompt += \"\"\"\n",
    "    ê° ë‰´ìŠ¤ì— ëŒ€í•´ êµ­ë‚´ ìë™ì°¨ ì—…ê³„ì˜ ì…ì¥ì—ì„œ ê°ì •ì„ íŒë‹¨í•˜ì„¸ìš”.\n",
    "\n",
    "    í˜•ì‹:\n",
    "    ë‰´ìŠ¤ 1: ê¸ì •\n",
    "    ë‰´ìŠ¤ 2: ë¶€ì •\n",
    "    ë‰´ìŠ¤ 3: ê¸ì •\n",
    "    ë‰´ìŠ¤ 4: ë¶€ì •\n",
    "    ë‰´ìŠ¤ 5: ê¸ì •\n",
    "\n",
    "    - ì˜¤ì§ ìœ„ì˜ í˜•ì‹ë§Œ ì‚¬ìš©í•˜ê³ , ë‹¤ë¥¸ ë¶ˆí•„ìš”í•œ ë¬¸ì¥ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    - ë§Œì•½ ê°ì •ì´ ì• ë§¤í•˜ë©´ 'ë‰´ìŠ¤ X: ì¤‘ë¦½'ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    - ì–´ë–¤ ê²½ìš°ì—ë„ 'íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ëŠ” ë‹µë³€ì€ í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"ğŸŸ¢ GPT ìš”ì²­ ì‹œì‘...\")\n",
    "        response = await asyncio.wait_for(\n",
    "            client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            ),\n",
    "            timeout=timeout\n",
    "        )\n",
    "        raw_output = response.choices[0].message.content.strip()\n",
    "\n",
    "        # âœ… GPT ì‘ë‹µ ì›ë³¸ ì¶œë ¥\n",
    "        print(f\"ğŸ”µ GPT ì›ë³¸ ì‘ë‹µ: {raw_output}\")\n",
    "\n",
    "        # âœ… ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ê°ì • ê²°ê³¼ ì •í™•í•˜ê²Œ ì¶”ì¶œ\n",
    "        sentiments = []\n",
    "        pattern = r\"ë‰´ìŠ¤\\s*(\\d+)\\s*:\\s*(ê¸ì •|ë¶€ì •|ì¤‘ë¦½)\"\n",
    "        matches = re.findall(pattern, raw_output)\n",
    "\n",
    "        # âœ… ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë§¤í•‘í•˜ì—¬ ìˆœì„œ ìœ ì§€\n",
    "        matches_dict = {int(num): sentiment for num, sentiment in matches}\n",
    "\n",
    "        for i in range(1, len(news_list) + 1):\n",
    "            sentiments.append(matches_dict.get(i, \"N/A\"))\n",
    "\n",
    "        return sentiments\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"âš ï¸ GPT ì‘ë‹µ ì§€ì—°: íƒ€ì„ì•„ì›ƒ ë°œìƒ\")\n",
    "        return [\"ERROR\"] * len(news_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ê°ì • ë¶„ì„ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰\n",
    "async def process_dataframe_batch(df, batch_size=5):\n",
    "    results = []\n",
    "    \n",
    "    print(\"ğŸ”µ ë°°ì¹˜ ê°ì • ë¶„ì„ ì‹œì‘...\")\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        print(f\"ğŸŸ¢ ë°°ì¹˜ {i//batch_size + 1} ì‹¤í–‰ ì¤‘...\")\n",
    "\n",
    "        batch = df.iloc[i : i + batch_size]\n",
    "        batch_news_list = [(str(row[\"id\"]), row[\"content\"]) for _, row in batch.iterrows()]\n",
    "        \n",
    "        try:\n",
    "            sentiments = await batch_analyze_sentiment_gpt4o(batch_news_list)\n",
    "            print(f\"âœ… ë°°ì¹˜ {i//batch_size + 1} ì‘ë‹µ ì™„ë£Œ: {sentiments}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ GPT ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            sentiments = [\"ERROR\"] * len(batch_news_list)\n",
    "        \n",
    "        results.extend(sentiments)\n",
    "        await asyncio.sleep(0.3)\n",
    "\n",
    "    print(\"ğŸ”µ ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    df[\"sentiment_gpt\"] = results\n",
    "    return df\n",
    "\n",
    "# âœ… ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ ê°€ì ¸ì™€ì„œ ì‹¤í–‰ (Google Colabì— ì í•©)\n",
    "loop = asyncio.get_event_loop()\n",
    "df = loop.run_until_complete(process_dataframe_batch(df))\n",
    "\n",
    "# âœ… ê°ì • ë¶„ì„ ê²°ê³¼ ìˆ«ìë¡œ ë³€í™˜\n",
    "sentiment_mapping = {\"ê¸ì •\": 1, \"ë¶€ì •\": -1, \"ì¤‘ë¦½\": 0}\n",
    "\n",
    "df[\"sentiment_gpt\"] = df[\"sentiment_gpt\"].map(sentiment_mapping)\n",
    "df[\"sentiment_gpt\"] = df[\"sentiment_gpt\"].fillna(0).astype(int)\n",
    "\n",
    "# MySQL ì—…ë°ì´íŠ¸\n",
    "update_query = text(\"UPDATE cleaned_news_data SET sentiment_gpt = :sentiment_gpt WHERE id = :id\")\n",
    "update_data = [{\"sentiment_gpt\": row[\"sentiment_gpt\"], \"id\": row[\"id\"]} for _, row in df.iterrows()]\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(update_query, update_data)\n",
    "\n",
    "print(\"âœ… MySQL ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ì„±ëŠ¥ í‰ê°€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
